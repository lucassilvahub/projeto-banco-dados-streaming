import asyncio
import json
from aiokafka import AIOKafkaConsumer
import logging
from elasticsearch import AsyncElasticsearch
from datetime import datetime
import os
import sys
import time
import re

# ========================
# üõ†Ô∏è Configura√ß√µes
# ========================
KAFKA_BROKER = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "kafka:9092")
TOPICOS = ["user_events", "response_events"]
ES_HOST = os.getenv("ELASTICSEARCH_HOST", "elasticsearch:9200")
LOG_FILE = "api_logs.json"  # Arquivo de fallback renomeado
KAFKA_CONSUMER_TIMEOUT_MS = int(os.getenv("KAFKA_CONSUMER_TIMEOUT_MS", "30000"))

# ========================
# üì• Configura√ß√£o de Logs
# ========================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("s3")

async def log_evento(es, evento, origem):
    """Registra o evento no Elasticsearch com √≠ndices din√¢micos baseados no evento"""
    
    # Determinar qual √≠ndice usar com base no tipo de evento e origem
    event_type = evento.get("event_type", "unknown")
    transaction_id = evento.get("correlation_id", "N/A")
    
    # Fun√ß√£o para obter o nome do √≠ndice dinamicamente
    def get_index_name():
        # Para eventos de resposta, extrair o tipo original
        if origem == "response_events" and evento.get("original_event_type"):
            # Usamos o tipo original para categorizar as respostas
            base_type = evento.get("original_event_type", "unknown")
            return f"api_{base_type}_responses"
        
        # Para eventos de requisi√ß√£o, usar o event_type diretamente
        elif origem == "user_events":
            # Extrair o nome da "entidade" do event_type - normalmente o primeiro substantivo
            # Por exemplo: "criar_usuario" -> "usuario", "atualizar_assinatura" -> "assinatura"
            if '_' in event_type:
                # Tenta extrair o nome da entidade ap√≥s o verbo
                entity = event_type.split('_', 1)[1]  # Pega a segunda parte ap√≥s o primeiro _
                # Se a entidade tiver outro verbo, pega apenas o substantivo
                if '_' in entity:
                    entity = entity.split('_')[0]  # Pega o substantivo principal
                return f"api_{entity}"
            else:
                # Fallback para o tipo de evento completo
                return f"api_{event_type}"
        
        # Fallback para qualquer outro caso
        return "api_events"
    
    # Obter o nome do √≠ndice
    index_name = get_index_name()
    
    # Adequar o formato do nome do √≠ndice para Elasticsearch (lowercase, sem caracteres especiais)
    index_name = re.sub(r'[^a-z0-9_]', '', index_name.lower())
    
    # Garantir que o √≠ndice tenha o prefixo "api_"
    if not index_name.startswith("api_"):
        index_name = f"api_{index_name}"
    
    # Criar o objeto de log
    log_entry = {
        "timestamp": datetime.utcnow().isoformat(),
        "transaction_id": transaction_id,
        "event_type": event_type,
        "original_event_type": evento.get("original_event_type", event_type),
        "source": evento.get("source", "N/A"),
        "target": evento.get("target", "N/A"),
        "status": evento.get("status", "N/A"),
        "message": evento.get("message", "N/A"),
        "topic": origem,
        "payload": evento  # Mant√©m o evento completo para refer√™ncia
    }
    
    # Tenta enviar para o Elasticsearch
    if es:
        try:
            # Verificar se o √≠ndice existe, se n√£o, cri√°-lo
            if not await es.indices.exists(index=index_name):
                await create_dynamic_index(es, index_name)
            
            await es.index(index=index_name, document=log_entry)
            logger.info(f"üìù API Log registrado no Elasticsearch ({index_name}): {event_type}")
            
            # Atualizar ou criar alias automaticamente
            await update_aliases(es, index_name)
            
            return
        except Exception as e:
            logger.error(f"‚ùå Erro ao registrar no Elasticsearch: {e}")
            # Continua para o fallback
    
    # Fallback: registra em arquivo local
    try:
        with open(LOG_FILE, "a") as log_file:
            log_file.write(json.dumps(log_entry) + "\n")
        logger.info(f"üìù API Log registrado em arquivo local: {event_type}")
    except Exception as e:
        logger.error(f"‚ùå Erro ao registrar em arquivo: {e}")

async def create_dynamic_index(es, index_name):
    """Cria um novo √≠ndice com mapeamento padronizado de forma din√¢mica"""
    logger.info(f"üõ†Ô∏è Criando dinamicamente o √≠ndice {index_name}...")
    
    # Mapeamento padr√£o para todos os √≠ndices de API
    mapping = {
        "properties": {
            "timestamp": {"type": "date"},
            "transaction_id": {"type": "keyword"},
            "event_type": {"type": "keyword"},
            "original_event_type": {"type": "keyword"},
            "source": {"type": "keyword"},
            "target": {"type": "keyword"},
            "status": {"type": "keyword"},
            "message": {"type": "text"},
            "topic": {"type": "keyword"},
            "payload": {"type": "object", "enabled": False}
        }
    }
    
    # Criar o √≠ndice
    await es.indices.create(index=index_name, mappings=mapping)
    logger.info(f"‚úÖ √çndice {index_name} criado com sucesso")

async def update_aliases(es, index_name):
    """Atualiza aliases baseado no nome do √≠ndice para facilitar pesquisas"""
    try:
        # Determinar em quais aliases esse √≠ndice deve ser inclu√≠do
        alias_actions = []
        
        # Todos os √≠ndices v√£o para o alias geral
        alias_actions.append({"add": {"index": index_name, "alias": "api_all_logs"}})
        
        # √çndices de resposta v√£o para o alias de respostas
        if "responses" in index_name:
            alias_actions.append({"add": {"index": index_name, "alias": "api_responses"}})
        # Outros √≠ndices v√£o para o alias de requisi√ß√µes
        else:
            alias_actions.append({"add": {"index": index_name, "alias": "api_requests"}})
        
        # Extrair o tipo da entidade (ex: usuarios, assinaturas, etc)
        entity_match = re.match(r'api_([^_]+)', index_name)
        if entity_match:
            entity_type = entity_match.group(1)
            # Criar um alias para todos os √≠ndices dessa entidade
            entity_alias = f"api_{entity_type}_all"
            alias_actions.append({"add": {"index": index_name, "alias": entity_alias}})
        
        # Executar as a√ß√µes de alias
        if alias_actions:
            await es.indices.update_aliases({"actions": alias_actions})
            
    except Exception as e:
        logger.error(f"‚ùå Erro ao atualizar aliases: {e}")

async def verificar_correlacao(evento_resposta):
    """Verifica e registra a correla√ß√£o entre request e response"""
    if evento_resposta.get("event_type") == "response" and "correlation_id" in evento_resposta:
        transaction_id = evento_resposta["correlation_id"] 
        original_type = evento_resposta.get("original_event_type", "desconhecido")
        status = evento_resposta.get("status", "desconhecido")
        
        logger.info(f"üîÑ Ciclo completo para transaction_id: {transaction_id}")
        logger.info(f"   Evento original: {original_type}")
        logger.info(f"   Status: {status}")

async def connect_to_elasticsearch(max_retries=15, retry_delay=5):
    """Conecta ao Elasticsearch com v√°rias tentativas."""
    es = None
    retries = 0
    
    while es is None and retries < max_retries:
        try:
            logger.info(f"üîå Tentando conectar ao Elasticsearch em {ES_HOST}... ({retries+1}/{max_retries})")
            es = AsyncElasticsearch([f"http://{ES_HOST}"])
            
            # Verificar a conex√£o
            if await es.ping():
                logger.info("‚úÖ Conex√£o com Elasticsearch bem-sucedida!")
                
                # Configurar os aliases principais se n√£o existirem
                base_aliases = ["api_all_logs", "api_requests", "api_responses"]
                
                for alias in base_aliases:
                    # Verificar se o alias j√° existe
                    try:
                        alias_exists = await es.indices.exists_alias(name=alias)
                        if not alias_exists:
                            # Criar um √≠ndice template para permitir cria√ß√£o din√¢mica de √≠ndices
                            await es.indices.put_index_template(
                                name=f"{alias}_template",
                                body={
                                    "index_patterns": ["api_*"],
                                    "template": {
                                        "mappings": {
                                            "properties": {
                                                "timestamp": {"type": "date"},
                                                "transaction_id": {"type": "keyword"},
                                                "event_type": {"type": "keyword"},
                                                "original_event_type": {"type": "keyword"},
                                                "source": {"type": "keyword"},
                                                "target": {"type": "keyword"},
                                                "status": {"type": "keyword"},
                                                "message": {"type": "text"},
                                                "topic": {"type": "keyword"},
                                                "payload": {"type": "object", "enabled": False}
                                            }
                                        }
                                    }
                                }
                            )
                            logger.info(f"‚úÖ Template para √≠ndices api_* criado")
                    except Exception as e:
                        logger.error(f"‚ùå Erro ao verificar/criar alias {alias}: {e}")
                
                return es
            else:
                logger.warning("‚ö†Ô∏è Elasticsearch respondeu, mas a conex√£o falhou.")
                retries += 1
                await asyncio.sleep(retry_delay)
        except Exception as e:
            logger.error(f"‚ùå Erro ao conectar ao Elasticsearch: {e}")
            retries += 1
            await asyncio.sleep(retry_delay)
    
    logger.warning("‚ö†Ô∏è N√£o foi poss√≠vel conectar ao Elasticsearch. Usando fallback para logs em arquivo local.")
    return None

async def create_kafka_consumer(max_retries=15, retry_delay=5):
    """Inicia o consumidor Kafka com v√°rias tentativas."""
    consumer = None
    retries = 0
    
    while consumer is None and retries < max_retries:
        try:
            logger.info(f"üîå Conectando ao Kafka em {KAFKA_BROKER}... ({retries+1}/{max_retries})")
            consumer = AIOKafkaConsumer(
                *TOPICOS,
                bootstrap_servers=KAFKA_BROKER,
                value_deserializer=lambda v: json.loads(v.decode("utf-8")),
                group_id="s3_logger_group",
                auto_offset_reset="latest",
                consumer_timeout_ms=KAFKA_CONSUMER_TIMEOUT_MS
            )

            await consumer.start()
            logger.info("‚úÖ Consumer Kafka iniciado com sucesso!")
            
            # Verificar os t√≥picos dispon√≠veis
            topics = await consumer.topics()
            logger.info(f"üìã T√≥picos dispon√≠veis no Kafka: {', '.join(topics)}")
            
            # Verificar se os t√≥picos necess√°rios existem
            missing_topics = [topic for topic in TOPICOS if topic not in topics]
            if missing_topics:
                logger.warning(f"‚ö†Ô∏è T√≥picos n√£o encontrados: {', '.join(missing_topics)}")
                # N√£o √© necess√°rio criar os t√≥picos aqui, pois o S2 j√° far√° isso
            
            return consumer
        except Exception as e:
            logger.error(f"‚ùå Erro ao iniciar consumer Kafka: {e}")
            retries += 1
            await asyncio.sleep(retry_delay)
    
    if consumer is None:
        logger.error("‚ùå N√£o foi poss√≠vel iniciar o Kafka Consumer ap√≥s v√°rias tentativas.")
        return None

async def monitorar_eventos():
    """Fun√ß√£o principal para monitorar eventos."""
    logger.info("üîÑ Iniciando servi√ßo de monitoramento de API S3...")
    
    # Esperando para os servi√ßos estarem prontos
    logger.info("‚è≥ Aguardando servi√ßos estarem dispon√≠veis...")
    await asyncio.sleep(10)  # Aguarda um pouco antes de tentar conectar
    
    # Tentar conectar ao Elasticsearch
    es = await connect_to_elasticsearch()
    
    if not es:
        logger.warning("‚ö†Ô∏è Usando fallback para logs em arquivo local")
        # Garantir que o arquivo de log existe
        if not os.path.exists(LOG_FILE):
            with open(LOG_FILE, "w") as f:
                f.write("")

    # Iniciar consumer Kafka
    consumer = await create_kafka_consumer()
    if not consumer:
        if es:
            await es.close()
        logger.error("‚ùå N√£o foi poss√≠vel iniciar o monitoramento sem conex√£o com o Kafka.")
        return False
    
    try:
        logger.info(f"üì° Monitorando t√≥picos para logs de API: {', '.join(TOPICOS)}")
        async for msg in consumer:
            evento = msg.value
            topico = msg.topic
            
            # Registrar o evento no Elasticsearch ou no fallback
            await log_evento(es, evento, topico)
            
            # Verificar correla√ß√£o entre request e response
            if topico == "response_events":
                await verificar_correlacao(evento)
    except Exception as e:
        logger.error(f"‚ùå Erro ao processar mensagens: {e}")
        return False
    finally:
        logger.info("üõë Fechando conex√µes...")
        if 'consumer' in locals():
            await consumer.stop()
        if es:
            await es.close()
    
    return True

async def main():
    """Fun√ß√£o principal com retry para garantir a execu√ß√£o."""
    retry_count = 0
    max_retries = 5
    
    while retry_count < max_retries:
        success = await monitorar_eventos()
        if success:
            break
        
        retry_count += 1
        logger.warning(f"‚ö†Ô∏è Tentativa {retry_count}/{max_retries} falhou. Tentando novamente em 10 segundos...")
        await asyncio.sleep(10)
    
    if retry_count == max_retries:
        logger.error("‚ùå N√∫mero m√°ximo de tentativas excedido. Encerrando servi√ßo.")
        sys.exit(1)

# ========================
# üöÄ Ponto de Entrada
# ========================
if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("üõë Servi√ßo interrompido pelo usu√°rio")
    except Exception as e:
        logger.error(f"‚ùå Erro fatal: {e}")
        sys.exit(1)